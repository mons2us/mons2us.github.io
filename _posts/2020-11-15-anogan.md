---
layout: post
title:  "Unsupervised Anomaly Detection with Generative Adversarial Networks to Guide Marker Discovery"
date:   2020-11-14 14:25:52
author: mons2us
categories: Paper-Reproduction Deeplearning
tags: deeplearning gan
---

   이 글은 AnoGAN을 이해하고자 다양한 소스로부터 학습한 내용을 정리한 글입니다. 혹시 내용 중 틀린 부분이 있거나 궁금한 사항이 있으신 경우 댓글을 남겨주시면 최선을 다해 소통하겠습니다.
   AnoGAN 구조를 재현한 전체 코드는 제 [깃허브](https://github.com/mons2us/paper_reproduction/tree/master/AnoGAN)를 참고하시기 바랍니다.


# Overview
이미지 데이터, 특히 의료 분야의 이미지 데이터에 있어서 다량의 레이블 된 데이터셋을 마련하기는 굉장히 어려운 일입니다. 특히 그것이 사람의 장기 사진이고, 정상인 경우를 (1), 결함이 있는 사진을 (0)으로 레이블링을 한다고 하면 두 클래스 간 불균형도 엄청날 겁니다. 공장에서도 마찬가지인데, 결함이 있는 데이터를 구하는 것 자체가 어렵기 때문입니다. 그렇다고 일부러 장기에 결함을 만들거나, 공정에서 불량품만 생산할 수도 없는 일이죠. 이뿐만 아니라 데이터를 제대로 레이블링 하는 것도 높은 수준으로 훈련된 전문가가 해야 할 일입니다. 레이블의 퀄리티에 따라서 모델의 성능 또한 좌지우지 될 수 있기 때문입니다.<br><br>
이러한 이유로 본 논문 <>의 저자는 anomaly를 찾는 데 있어 비지도 학습 방법을 제안하고 있고, 그것이 바로 AnoGAN입니다. 이름에서 알 수 있듯이 GAN을 기반으로 하는 모델인데요, 간단하게 요약하면
>(1) GAN으로 하여금 정상인 데이터만 배우도록 한다.<br>
(2) 정상 혹은 이상한(anomaly) 데이터(query)를 준다.<br>
(3) GAN이 이를 이용해 생성하는 데이터와 query를 비교해 anomaly score를 산출한다.<br>
(4) Anomaly score가 높으면 anomaly, 아니면 정상 데이터로 분류한다.<br>

입니다. 각 단계별 자세한 내용은 뒷 부분에서 설명하겠고, 우선 AnoGAN의 베이스라고 할 수 있는 GAN이 무엇인지에 대해서 알아야 하겠습니다.<br><br>

# GAN
[GAN](https://arxiv.org/abs/1406.2661)(shs은 2014년 우리의 좋은 친구 Ian Godfellow에 의해 제안된 방법론인데요, 워낙 유명한 방법론이라 다들 한번쯤은 들어보셨을 것이라 생각합니다. GAN은 아래와 같은 그림으로 굉장히 명료하게 표현될 수 있습니다.
![image](/assets/assets_post/2020-11-15-anogan/gan_1.png)<br>
아마 위 그림도 다들 보셨던 그림일 겁니다. GAN은 모델 내에 두 가지의 메인 모듈을 포함하고 있습니다. 하나는 생성자(Generator), 다른 하나는 구별자(Discriminator) 인데요, 그림처럼 생성자는 최대한 실제와 같은 데이터(위조 지폐)를 생성하고, 구별자는 생성자가 만든 데이터와 실제 데이터를 가지고 둘을 구별해 냅니다. 한번의 학습에서 구별자가 이겼다면(지폐를 위조한 게 들통남) 생성자는 다시금 좀 더 지폐 같은 위조 지폐를 만들어 냅니다. 이러한 과정이 반복되면 위조 지폐는 점점 실제 지폐와 유사해지고, 동시에 구별자는 위조 지폐를 구분해내는 데 더 능숙해지게 됩니다.<br><br>

사실 듣다보면 두개의 모델을 엮어 전체 네트워크를 구성한다는 점에서 샴 네트워크(siamese network)와 유사하다는 느낌을 받습니다. 다만 GAN에서의 모델(생성자 vs 구별자)이 서로 경쟁을 하며 학습을 진행하는 반면 샴 네트워크에서는 동일한 가중치를 공유하는 두개의 모델이 인풋 데이터가 같은 클래스인지를 학습한다는 차이점이 있습니다.<br><br>

GAN이 굉장히 획기적이고 다양한 응용 분야를 만들어 낸 방법론이지만, 문제는 학습에 있어서 안정성이 떨어진다는 데에 있습니다. 이론적으로는 converge하는 최적해가 존재하지만, 실제 적용에서는 그러한 가정이 깨지는 큰 단점을 갖고 있다고 합니다. 이를 보완하고자 등장한 다양한 파생 모델 중 가장 뛰어난 결과를 보여준 것이 [DCGAN](https://arxiv.org/abs/1511.06434)이고, AnoGAN 뿐 아니라 다양한 GAN 구조가 베이스를 두고 있는 모델이기도 합니다.<br>

# DCGAN
![image](/assets/assets_post/2020-11-15-anogan/dcgan_archi.png)
DCGAN은 기본적으로 GAN과 동일한 구조를 갖습니다. 생성자는 가짜 데이터를 만들고, 구별자는 가짜 데이터를 식별하도록 학습합니다. 다만 아래 다섯가지의 가장 큰 차이점을 지닙니다.
>(1) Max-pooling layer를 제거하고, 생성자는 fractional-strided convolution, 구별자는 strided convolution를 사용<br>
(2) Batch normalization 사용<br>
(3) Fully connected layer 제거<br>
(4) 생성자의 최종 활성함수로 Tanh을 사용하고, 나머지 층의 활성함수로 ReLU 사용<br>
(5) 구별자는 활성함수로 LeakyReLU 사용<br>
논문의 저자들이 GAN을 안정화시키는 위와 같은 방법을 찾기 위해 얼마나 많은 수고스러움을 겪었을 지는 상상도 되지 않습니다. 어찌 됐든 위 구조와 같이 CNN 기반의 GAN 모델을 구축하게 됨으로써 훨씬 고해상도의 결과물을 생성할 수 있게 되었다고 합니다.<br>
![image](/assets/assets_post/2020-11-15-anogan/dcgan_example2.png)<br>
실제 
 이 논문의 contribution은 한가지 더욱 중요한 점은 DCGAN의 저자들이 latent space에서 추출된 $$z$$가 지니는 의미를 이용하였다는 점입니다. 아래 그림을 한 번 살펴보겠습니다.<br><br>
![image](/assets/assets_post/2020-11-15-anogan/dcgan_example1.png)<br>
저자들은 위 그림을 "walking in the latent space"라고 표현합니다. 말 그대로 DCGAN이 학습한 latent space에서 샘플되는 z를 굉장히 조금씩 옮겨가면서(walk) 결과물을 생성함에 따라, 결과물이 굉장히 smooth하게 변하는 것을 볼 수 있습니다. 동일한 침실이지만 침대의 위치나 형상, 혹은 TV의 유무 등에서 변화가 조금씩 생기고 있죠. 이는 $$z$$에 맵핑되는 함수가 굉장히 안정적으로 학습이 되었다는 의미가 됩니다. 즉 DCGAN의 생성자는 학습에 사용한 침실 이미지들을 가지고 latent space인 $$Z$$ 공간에 분포를 그리게 되고, 해당 분포에서 하나의 점($$z$$)을 선택해 실제와 유사한 이미지로 맵핑하는 기능을 한다고 볼 수 있습니다. 이 때 예를 들어 $$z=[0.1, 0.2, 0.9]$$라고 했을 때, DCGAN의 저자들은 $$0.9$$가 "침대의 모양"을 의미함을 알 수 있고, 이를 $$0.89$$ 등으로 smooth하게 변화시킴에 따라 생성자의 결과물에서 침대의 모양이 조금씩 변화한다는 점에 주목할 수 있습니다. 만약 $$0.1$$이 "침실을 바라보는 방향"이라고 하면, 이 값을 $$0.9$$로 변화시켜서 반대 방향에서 바라보는 침실의 모습을 생성할 수도 있겠죠.<br><br>
사실 이 부분은 VAE(Variational Autoencoder)와 유사하다고 볼 수 있는데, VAE는 input 이미지를 latent space로 투영시키는 manifold를 학습한 후 샘플링을 통해 새로운 이미지로 복원시킵니다. 이 때 latent space에서 추출하는 sample의 값을 위와 마찬가지로 조금씩 변형시키면 굉장히 유사하지만 천천히 변화되는 이미지가 생성됩니다. 즉 DCGAN의 생성자가 결국 latent space의 한 값을 실제 있을 법한 이미지로 변화시킨다는 점에서 VAE의 decoder와 굉장히 유사하다고 볼 수 있습니다. 이에 관해 보다 자세한 내용은 제 블로그의 VAE 정리글을 살펴보시면 좋겠습니다.<br>

# AnoGAN
위 DCGAN을 이해한다면 AnoGAN을 이해하기는 어렵지 않습니다. 학습된 DCGAN을 그대로 가지고 와서, anomaly score를 산출하기 위한 학습만 추가하면 완성되기 때문입니다. 그럼 구조를 한 번 자세히 살펴보도록 하겠습니다.<br>

## Architecture
### (1) DCGAN
우선 DCGAN을 학습해야 합니다. 후술하겠지만 이 DCGAN은 "생성자+구분자"보다는 "정상적인 이미지를 바탕으로 __정상적인 이미지만 생성하는 생성자__"로 생각하시면 되는데, 논문을 읽다가 제 자신의 이해를 돕기 위해 만든 예시는 아래와 같습니다.(_이 예시를 가지고 AnoGAN도 설명하겠습니다_)<br>
>A군이 취업을 위해 TOEIC 공부를 하고 있습니다. 하지만 단순히 단어를 외우고 문법을 공부하는 것으로는 성에 차지 않았던 A군은 본인이 직접 TOEIC 문제를 만들어 풀어보는 방식으로 학습을 하고자 합니다. 이를 위해 A군은 TOEIC 기출 문제를 잔뜩 구해왔고, 본인이 만든 문제를 풀게 하기 위해 TOEIC 고득점자인 B군을 불렀습니다.<br>

위 예에서 A군은 DCGAN의 생성자로 볼 수 있습니다. 물론 DCGAN을 학습하기 위한 실제 데이터는 TOEIC 기출문제이고, B군은 A군이 만든 문제가 실제 나올법한 문제인지를 판단하는 구별자가 됩니다. A군은 문제를 만들 때 실제 기출문제만을 참고하기 때문에 몇 번의 반복 후에는 정말 기출문제와 비슷한 문제를 만들게 됩니다. 동시에 B군은 A군이 만든 기출문제가 점차 정교해지니, 실제 문제와 더 잘 구별하기 위해 본인 또한 점점 더 정교한 식별 능력을 키우게 됩니다.<br>

DCGAN을 학습하기 위한 코드는 아래와 같습니다. GAN이 굉장히 hyperparameter에 민감하다고 하던데, 실제로 층별로 layer의 개수나 optimizer의 parameter를 어떻게 바꾸느냐에 따라 수렴 속도, 혹은 애초에 수렴을 하는지 안하는지도 달라지는 것 같습니다.<br><br>

```python
## DCGAN
class Generator(nn.Module):

    def __init__(self,
                 image_size = CONFIGS.image_size,
                 z_dim = CONFIGS.z_dim,
                 g_dim = CONFIGS.g_dim,
                 channel = CONFIGS.bridge_channel):

        super(Generator, self).__init__()

        self.rat = int(image_size / 4)
        self.channel = channel

        self.z_dim = z_dim
        self.g_dim = g_dim

        self.g_fclayer = nn.Sequential(
            nn.Linear(z_dim, g_dim*8*self.rat*self.rat),
            nn.BatchNorm1d(g_dim*8*self.rat*self.rat),
            nn.ReLU()
        )
        
        self.g_convlayer = nn.Sequential(
            nn.ConvTranspose2d(g_dim*8, g_dim*4, 3, 2, 1, 1),
            nn.BatchNorm2d(g_dim*4),
            nn.LeakyReLU(),
            
            nn.ConvTranspose2d(g_dim*4, g_dim*2, 3, 1, 1),
            nn.BatchNorm2d(g_dim*2),
            nn.LeakyReLU(),

            nn.ConvTranspose2d(g_dim*2, g_dim, 3, 1, 1),
            nn.BatchNorm2d(g_dim),
            nn.LeakyReLU(),

            nn.ConvTranspose2d(g_dim, channel, 3, 2, 1, 1),
            nn.Tanh()
        )

    def forward(self, z):
        out = self.g_fclayer(z)
        out = out.view(out.shape[0], self.g_dim*8, self.rat, self.rat)
        out = self.g_convlayer(out)
        return out


class Discriminator(nn.Module):
    '''
    ConvTranspose2d(input_c, output_c, kernel_size, stride, padding)
    '''
    def __init__(self,
                 image_size = CONFIGS.image_size,
                 d_dim = CONFIGS.d_dim,
                 channel = CONFIGS.bridge_channel):

        super(Discriminator, self).__init__()

        self.rat = int(image_size / 4)
        self.d_dim = d_dim

        self.d_convlayer = nn.Sequential(

            nn.Conv2d(channel, d_dim, 3, stride = 1, padding = 1),
            nn.BatchNorm2d(d_dim),
            nn.LeakyReLU(),

            nn.Conv2d(d_dim, d_dim*2, 3, stride = 2, padding = 1),
            nn.BatchNorm2d(d_dim*2),
            nn.LeakyReLU(),

            nn.Conv2d(d_dim*2, d_dim*4, 3, stride = 2, padding = 1),
            nn.BatchNorm2d(d_dim*4),
            nn.LeakyReLU(),

            nn.Conv2d(d_dim*4, d_dim*8, 3, stride = 1, padding = 1),
            nn.BatchNorm2d(d_dim*8),
            nn.LeakyReLU()
        )
        
        self.d_fclayer = nn.Sequential(
            nn.Linear(d_dim*8*self.rat*self.rat, 1),
            nn.Sigmoid()
        )

    def forward(self, x):
        out = self.d_convlayer(x)
        out = out.view(out.shape[0], -1)
        med_feature = out
        out = self.d_fclayer(out)
        return out, med_feature
```

![image](/assets/assets_post/2020-11-15-anogan/anogan_archi1.png)<br>
출처: [물공's 딥러닝](https://sensibilityit.tistory.com/506)<br>

### (2) New image to Z
A군과 B군이 학습을 오래 진행하고 나면, 둘이 각자 맡은 부분에서 굉장히 뛰어난 능력을 발휘하게 됩니다. 문제는 A군이 할 수 있는 일이 $$G(z): z -> x$$에 한정된다는 겁니다. 즉 $$z$$가 "쉬운 문법", "어려운 독해" 등 TOEIC 문제의 latent space에서의 공간이라고 하면, 그것이 주어졌을 때 해당하는 (실제와 유사한)문제에 맵핑하여 자신만의 문제를 만든다는 것이죠. 이게 DCGAN만으로 학습했을 때의 한계점이라고 하면, AnoGAN은 이를 보완하여 반대 방향의 맵핑($$\mu (z): x -> z$$)을 찾고자 합니다. 이게 왜 중요한지는 뒷 부분에 설명드리기로 하고, 우선 이러한 맵핑을 새롭게 학습할 수 있는 구조를 보겠습니다.<br><br>

### (3) Anomaly Detection
결과적으로 우리는 잘 학습된 DCGAN을 가지고 있고, 이 때 query 이미지($$x$$)를 넣으면 그것과 가장 유사한 이미지 $$x`$$를 생성할 수 있는 latent space 상의 $$z$$를 찾을 수 있습니다. 이걸 anomaly detection에 어떻게 사용할까요?<br><br>
먼저 아무렇게나 $$z_{1}$$이라는 노이즈를 샘플링을 해서 DCGAN을 이용해 이미지를 생성합니다. DCGAN이 제대로 학습이 됐기 때문에 노말한 이미지에 가까운 무언가가 생성이 될 것입니다. 하지만 이건 query에 들어오는 이미지와는 좀 다를 수 밖에 없는게, MNIST를 예로 들면 숫자 1 사진을 넣었지만 generator는 0을 생성할 수가 있습니다. 요렇게 되면 우선 Residual Loss 자체가 높아지면서, 그에 따라 back-propagate 되는 gradient가 커집니다. 이게 맨 앞 latent space로 가면 $$z_{1}$$을 맵핑하는 함수에 대해 많은 수정이 가해지고, 그 후 $$z_{2}$$가 새로 샘플링됩니다.<br><br>
$$z_2$$는 좀 더 0보다는 1에 가까운 숫자를 표현(represent)하는 latent space 상의 한 부분이 됩니다. 이걸 이용해 generate는 1에 가까운 노말한 이미지를 뽑습니다. 이 과정을 계속 반복(약 500번)하면, 우리는 query에 들어가는 1에 최대한 가까운 이미지를 생성하고자 합니다. 이 때 우리가 참고할 수 있는 것은 이전에 학습한 latent space인데, 이 곳에 노말한 데이터들이 잔뜩 모여있기 때문입니다. 따라서 query가 노말하다면 iteration 안에 latent space에서 가장 유사한 정상 이미지로 맵핑하기는 어렵지 않습니다.<br><br>
반면 abnormal한 query가 들어왔다면, latent space에서 이와 유사한 이미지로 맵핑하기는 쉽지가 않습니다. latent space에 abnormal한 데이터의 분포는 학습되지 않았기 때문입니다. 어찌저찌 최대한 유사한 공간을 찾아 유사한 이미지로 맵핑(생성)을 했지만, 이전에 비해 그 차이가 커질 수밖에 없습니다. 이 때 anomaly score가 커지게 되고, 이것이 AnoGAN이 비지도 학습을 통해 anomaly를 찾아내는 방법이자 핵심입니다. (모의고사랑 비교)<br><br>
AnoGAN의 loss를 학습하여 anomaly score를 산출하는 코드는 아래와 같습니다.<br>
```python
# ---------------
#   AnoGAN Loss
# ---------------
def residual(x, z):
    _loss = torch.sum(torch.abs(x - z))
    return _loss

def anomaly_score(x, z, discriminator):
    
    dis_real, mediate_real = discriminator(x)
    dis_fake, mediate_fake = discriminator(z)

    ano_loss = residual(x, z)
    dis_loss = residual(mediate_real, mediate_fake)

    _anomaly_score = 0.9 * ano_loss + 0.1 * dis_loss

    return _anomaly_score


def train_anogan(model_g,
                 model_d,
                 inf_data,
                 inf_size,
                 lr = 0.0002,
                 max_iter = 500,
                 cuda = True):

    device = torch.device("cuda" if cuda else "cpu")

    model_g = model_g.to(device)
    model_d = model_d.to(device)

    # -----------------------------------
    #    Fix generator & discriminator
    # -----------------------------------
    model_g.eval()
    model_d.eval()

    z = init.normal_(torch.Tensor(inf_size, 100), mean = 0, std = 0.1).to(device)
    z = Variable(z, requires_grad = True)

    z_optimizer = optim.Adam([z], lr = lr)

    for i in tqdm(range(max_iter)):
        gen_fake = model_g(z)
        ano_loss = anomaly_score(Variable(inf_data).to(device), gen_fake, model_d)
        ano_loss.backward()

        z_optimizer.step()

        if (i+1) % 100 == 0:
            print(f"Anomaly Score: {ano_loss.data}")
```


# Conclusions
오늘 소개드린 논문 \<Unsupervised Anomaly Detection with Generative Adversarial Networks to Guide Marker Discovery\>는 의료 이미지 분야와 같이 라벨링이 어렵고, 특히 anomaly 데이터 자체가 수집이 어려운 환경에서 GAN에 기반한 비지도 학습으로 그러한 anomaly 데이터를 탐지하는 방법론을 제시하고 있습니다. DCGAN이 워낙 정상 데이터를 학습하고 생성하는 데에 뛰어난 모델이다보니, 이렇게 라벨링이 없는 환경에서도 뛰어난 성능을 보이는 모델이 탄생한 것이 아닌가 싶습니다. 본 포스팅은 MNIST 데이터를 가지고 implementation을 했지만, 시간이 생기면 CIFAR10을 가지고 실험을 해볼 생각입니다.<br><br>

긴 포스팅 읽어주셔서 너무나 감사합니다. 배울 것이 많은 학생이라 제가 해석한 내용 중 틀린 부분이나 보완이 필요한 부분이 있을 수 있습니다. 가르침 주시기 위한 피드백은 언제나 환영입니다.<br><br><br>