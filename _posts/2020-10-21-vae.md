---
layout: post
title:  "Autoencoding Variational Bayes"
date:   2020-10-10 14:25:52
author: mons2us
categories: Deeplearning
tags: deeplearning dimensionality-reduction
---

   이 글은 Variational Autoencoder를 이해하고자 다양한 소스로부터 학습한 내용을 정리한 글입니다. 혹시 내용 중 틀린 부분이 있거나 궁금한 사항이 있으신 경우 댓글을 남겨주시면 최선을 다해 소통하겠습니다.
   VAE 구조를 재현한 전체 코드는 제 [깃허브](https://github.com/mons2us)를 참고하시기 바랍니다.

# Overview
Variational Autoencoder(VAE)가 무엇일까요?<br>
사실 오토인코더라는 단어는 어느 정도 친숙합니다만, VAE는 논문을 제대로 읽어본 적도, 또는 관련 내용을 정리해 본 적도 없었습니다. 이번에 한 번 정리를 해볼 기회가 생겨 제대로 논문을 읽어보자마자 식은 땀을 흘렸습니다. 딥러닝에 깊은 지식이 없는 까닭일 수도 있습니다만, 오래 전 통계학을 공부할 때 잠시 접했던 수많은 확률.. 그리고 수식들이 있었기 때문일 겁니다. 이후 자리를 잡고 앉아 정말 다양한 영상 강의, 블로그 등을 샅샅이 뒤지면서 내용을 정리했고, 이 글을 접하는 분들께서 최대한 쉽게 이해하실 수 있도록 정리를 해보았습니다. 물론 아직도 깊게 팔수록 어려운 내용이 남아있기 때문에, 해당 부분들은 추가적으로 학습하여 따로 포스팅을 해보도록 하겠습니다.<br><br>
VAE는 2013년 논문 [Autoencoding Variational Bayes](https://arxiv.org/abs/1312.6114)에서 제안되었고, 당시 generative model로서 굉장히 센세이션을 일으켰다고 합니다. VAE와 구조적으로 매우 유사한 autoencoder 또한 이미지 복원 등에 상당히 획기적인 기술입니다만, deterministic한 특성으로 모델의 latent space가 노이즈에 취약하다는 약점 등이 있습니다. VAE는 이 구조에 '확률'을 도입함으로써 노이즈에 어느 정도 대응책을 마련했고, 특히 genrative model의 관점에서 상당히 좋은 성능을 보인다고 합니다.<br><br>
다만 VAE를 학습하기에 앞서, 먼저 VAE와 구조적으로 매우 유사한 autoencoder가 무엇인지 알아야 하겠습니다.<br><br>

## Autoencoder
Autoencoder는 한번쯤은 들어봤을, 그리고 공부를 해보셨을 내용이라 간단하게만 짚고 넘어가겠습니다.<br>
Autoencoder는 쉽게 말해 데이터 $$x$$로부터 latent variable인 $$z$$를 샘플링하고, $$z$$로부터 다시 $$z$$를 샘플링하는 일련의 과정입니다. 여기서 '샘플링'은 '맵핑'의 의미로 생각하시면 될텐데, 위 내용은 다시말해 $$X$$공간의 어떤 값을 $$Z$$공간(latent space)의 어떤 값으로 맵핑하고, 그 값을 다시 $$X'$$ 공간의 값으로 맵핑(reconstruct)하는 것을 의미합니다. 구조는 아래와 같습니다.<br><br>
![image](/assets/assets_post/2020-10-21-vae/autoencoder_archi.png)<br><br>
다만 이 때 대부분의 경우 아래의 그림과 같이, $$Z$$공간은 $$X$$공간에 비해 저차원입니다. 따라서 autoencoder의 encoder가 하는 일이 곧 "Manifold Learning"과 동일한 맥락이라고 할 수 있습니다.<br>

![img-manifold](https://dmm613.files.wordpress.com/2014/12/manifold.png)<br>

Autoencoder는 사실 굉장히 간단합니다. 양쪽에 데이터($$X, X'$$)를 두고, 가운데 layer를 구축한 후에 복원되는 데이터($$X'$$)가 원본 데이터($$X$$)와 최대한 같아지도록 layer를 학습시키면 됩니다.
그러나 위와 같은 알고리즘의 결과물은 말그대로 '복원'의 의미가 강합니다. $$X$$와 $$X'$$이 최대한 같아지도록 학습을 하면, latent 공간 $$Z$$에서 새로운 데이터 공간 $$X'$$로의 맵핑이 deterministic한, 'cheat'에 가까운 되어버리겠죠. 이 때 latent variable을 조금만 변형하면?<br>
예를 들어 [한국], [일본]과 같은 단어들($$X$$)을 $$[-1.5, 1.7], [-1.0, 2]$$라는 저차원의 값으로 맵핑하는 autoencoder가 있을 때, Z값을 $$[-1.3, 1.9]$$로 부여하여 $$X'$$으로 샘플링하면 [하본]처럼 이상한, 현실에서 존재하지 않을 법한 결과물이 생성될 수 있습니다. 이는 generative model의 관점에서는 심각한 한계점이 됩니다. Variational Autoencoder는 이러한 한계점을 확률론적 관점에서 해결해보고자 등장한 이론입니다. 실제로 효과도 뛰어나고, 한번쯤 제대로 공부해 봐야 할 재밌는 이론이라고 생각합니다.<br>

---

<br><br>

# Variational Autoencoder
## **Introduction**
앞서 설명했듯이 Autoencoder는 input 데이터의 manifold를 잘 학습시키는 데에 초점을 둡니다. 그렇기 때문에 input 데이터를 좀 더 낮은 차원(Z-공간)에서의 어떠한 값(latent variable)로 맵핑하는 "차원 축소"가 주 기능이라고도 볼 수 있습니다.<br>
<br>
반면 VAE는 차원의 축소보다는 "생성"의 기능을 합니다. autoencoder가 데이터의 특성을 어떠한 값으로 맵핑하는 데에 초점을 맞추기 때문에 랜덤한(_현실적인_) latent 값에 대해서는 이상한 결과물을 뱉는다고 말씀드렸습니다. VAE는 이러한 Autoencoder에 좀 더 랜덤성, 즉 "변동"을 집어넣음으로써 보다 현실적으로 $$z$$ ~ $$X'$$을 구현할 수 있는 모델이라고 할 수 있겠습니다.<br>
<br>
아래 그림([출처](https://www.jeremyjordan.me/variational-autoencoders/))을 보시면, 사람의 얼굴에서 encoding된 "웃음"이라는 latent 공간에서의 표현(code)이 AE와 VAE 간 어떻게 다른 지 직관적으로 파악할 수 있습니다. (_좌: AE, 우: VAE_)<br>

![image](/assets/assets_post/2020-10-21-vae/latent_with_prob.png)<br>
즉 autoencoder가 인코딩한 code $$z$$는 '값', VAE가 인코딩한 code $$z$$는 '분포'로 생각하면 될 것 같습니다. 그럼 decoding을 할 때 (1) <u>autoencoder는 값으로부터 데이터를 복원</u>하는 데 반해 (2) <u>VAE는 분포로부터 샘플링</u>을 하기 때문에, 해당 분포가 적절하다는 가정 하에 "generative"의 목적에 좀 더 맞는 모형은 VAE가 될 것 같군요. VAE의 구조를 조금 더 자세히 살펴보겠습니다.
<br>
<br>

## **Architecture**

![image](/assets/assets_post/2020-10-21-vae/vae_architecture.png)
왜 Autoencoder라는 이름이 붙었는지, 위 도식을 보면 알 수 있습니다. (_구조적으로 굉장히 유사합니다_) Input 데이터가 들어오면, 어떠한 방식으로 $$z$$라는 latent feature로 표현하게 되고, 이를 Decoder를 통해 새로운 이미지로 reconstruct 합니다.<br><br>
다만 기본적인 autoencoder와 가장 큰 차이점은 바로 encoder의 결과물입니다. Autoencoder는 그냥 데이터를 layer들을 거쳐 축소($$x \rightarrow z$$) 학습을 하는 반면, VAE는 무언가 분포의 파라미터처럼 보이는 값들을 생성합니다. 비슷한 이름이나, "Variational"이라는 단어가 두 모델 간 이런 차이를 만들어 내는 것 같습니다. 그렇다면 "Variational"이 뭘까요?<br><br>

## **왜 "Variational" 일까?** <br>

Variational이란 사전적으로 "변동의"를 의미하며, 조금 다른 관점에서 본다면 "확률론적으로 정해진"이라고도 볼 수 있습니다. 우리가 배우는 기초적인 선형회귀만 보더라도, 최종적으로 적합하고자 하는 모델은 "확률적인 분포를 가정하고 데이터 X의 변동"을 제일 잘 설명하는 회귀선이 됩니다. VAE 또한 유사한 맥락에서 접근할 수 있습니다. 뉴럴 네트워크를 사용해서 데이터를 latent 공간으로 deterministic하게 맵핑하는게 아니라, "변동"을 집어 넣어서 보다 현실적으로 이미지를 복원하는 generative model을 구축하려고 하는 것입니다. 여기서 VAE를 이해하는 데 가장 중요한 Variational Inference가 등장합니다.
<br><br>
## **Variational Inference**<br>
Variational Inference에 대한 설명을 시작하기에 앞서, 우리는 $$p(x\mid z), p(z\mid x)$$와 같은 확률에 대한 기본 개념을 알아야 합니다. (_사실 제가 맨날 읽어도 맨날 까먹기 때문에 정리하는 의미가 강합니다._)<br>
<br>

---
### **기초적인 확률에 대한 정리**
간단하게, 이미지 데이터 공간인 $$X$$를 제가 오늘 A대학교 공과대학을 지나면서 본 커피의 집합이라고 해봅시다. 그럼 $$x_1$$은 아침에 본 커피겠고, $$x_{20}$$은 점심 때 쯤에 본 커피, ...가 되겠습니다.<br>
<br>
이 때 마침아래와 같은 조건이 주어진다고 해봅시다.<br>
* A대학교 공과대학 주변에는 커피집이 세 개 밖에 없습니다. 커피가 있다면, 무조건 세 집중에 하나에서 구매한 커피가 됩니다.<br>
* 다만 저는 공과대 학생들이 어느 커피집에서 커피를 많이 사는지 잘 모릅니다.

이러한 상황을 이용해서 몇가지 확률적 개념을 알아보고, 이를 VAE와 연관시켜보도록 하겠습니다.<br>
<br>
(1) $$p(y))$$<br>
'커피집의 확률'이자, prior(사전확률)입니다. '사전확률'이라는 것은 사건(지나가다 커피 하나를 봄)이 일어나기 전에 이미 알려진 확률이라는 의미입니다. 다만 위의 예에서는 제가 학생들이 선택하는 커피집의 확률 분포를 모르기 때문에 저는 모르는 확률이 되겠죠. 현실적으로는 이 사전확률을 모르는 경우가 굉장히 많습니다.<br><br>
VAE 구조에서 decoder가 latent feature인 $$z$$를 이용해 데이터 $$x$$를 생성한다고 할 때, decoder가 이용하는 분포는 $$p(x\mid z)$$가 되겠고 prior는 $$z$$가 됩니다. 만약 우리가 이 $$z$$를 안다면, $$x$$를 생성하는 건 굉장히 쉽겠죠.<br><br>

(2) $$p(y\mid x)$$<br>
말 그대로 커피 하나를 봤을 때 "저게 어느 커피집에서 산 커피일지" 하는 확률입니다. 이를 사건이 이미 발생한 이후 추론되는 확률이라 하여 posterior(사후확률)이라고 합니다.<br><br>
VAE 구조를 보기 전에, 만약 커피의 이미지 자체를 복원해본다고 해볼까요? 제가 본 커피가 보다 작은 차원의 latent space에 있는 $$[컵의 높이, 컵의 너비, 내용물의 색]$$이라는 latent feature($$z$$)로 맵핑된다고 해봅시다. 그럼 $$p(z\mid x)$$는 이미지가 주어졌을 때 과연 어떤 latent feature로 맵핑이 될까? 하는 사후확률이 되겠습니다. 눈치 채셨겠지만, 이게 VAE의 구조에서 encoder가 사용하는 분포가 됩니다.<br><br>

(3) $$p(x\mid y)$$ <br>
그 유명한 우도(likelihood)입니다. $$y$$의 분포를 가정했을 때 "그로부터 $$x$$가 어느 확률로 발생하는지"라는 의미로 해석 할 수 있습니다. 만약 커피집 a가 거의 상권을 독점하여 $$[a, b, c]$$의 비율이 $$[0.99, 0.05, 0.05]$$라고 하면, 해당 분포에서 $$p(X=x_a\mid y)(=likelihood)$$은 굉장히 높겠죠. 여기서 한 발 더 나아가면 MLE(Maximum Likelihood Estimation)가 있는데, 쉽게 말하면 $$z$$의 분포를 가정했을 때 그 분포의 파라미터 $$\theta$$에 대해 $$x$$라는 데이터가 관측되었을 확률을 최대로 만드는 $$\theta'$$을 추정하는 방법입니다.<br><br>
VAE의 구조에서 $$p(x\mid z)$$는 decoder로 볼 수 있습니다. 다만 우리가 $$p(z)$$를 모르기 때문에, 후술하겠지만 $$p(z)\sim N(0,1)$$을 가정하고 문제를 풀게 됩니다. 사실 MLE가 대부분의 머신러닝/딥러닝에서 쓰이는 굉장히 중요한 개념인데, 요거는 따로 포스팅을 할 계획입니다. 제대로 공부하고 설명하려면 한 장으로는 부족할 듯 싶습니다.

___
<br>
지금까지 VAE를 이해하는 데 필요한 확률 개념에 대해 간단하게 설명을 해보았습니다. 더 깊이 들어갈수록 추가할 내용은 많지만 나중에 제대로 포스팅을 하는 것으로 하고... Variational Inference로 돌아가도록 하겠습니다.<br><br>

VAE 관점에서, 우리가 게임 그래픽 디자이너이고 맵에 흩뿌려질 수많은 나무를 디자인해야 한다고 해볼까요? 이 때 나무의 이미지 $$x$$에 대해 일일히 $$[x_{1}, x_{2}, ..., x_{n}]$$을 만들어내는 것은 참 수고스러운 일이겠죠. 만약 나무를 [잎의 색, 몸통의 너비, 키]와 같이 굉장히 간단한 벡터 $$z$$로 표현할 수 있다고 하면 우리는 이 $$z$$가 참 탐날 겁니다. $$z$$만 알면, 나머지는 그냥 샘플링을 통해 현실에 있을 법한 나무들을 알아서 생성해 주기 때문이죠. 일이 바쁜 디자이너로서 우리가 알고자 하는 것은 $$p(z\mid x)$$로, "$$x$$만을 관찰할 수 있는 환경에서 $$z$$의 특징을 추정해보자"라는 겁니다. 하지만,<br>
\begin{align}p(z\mid x) = \frac {p(x\mid z)p(z)} {p(x)}\end{align}<br>
에서 우리는 $$p(x)$$를 알 수 없습니다. 이미지의 확률분포를 정확히 알지 못하기 때문입니다. 표현을 아래과 같이 조금 달리 해봐도,<br>
\begin{align}p(x) =  \int_{z}p(x\mid z)p(z)dz\end{align}
이는 다루기 힘든(intractable) 값입니다. 만약 $$z$$가 차원이 좀만 높아져도 적분을 수도 없이 많이 하게 됩니다. 그렇다면 $$p(z\mid x)$$를 어떻게 구할 수 있을까요? 여기서 Variational Inference가 등장해 $$p(z\mid x)$$를 '근사'할 수 있는 방법을 찾습니다.<br><br>

### **Assume $$q(z\mid x) \sim p(z\mid x)$$**<br>
우리가 구하려는 사후확률 $$p(z\mid x)$$를 어떤 분포 $$q(z)$$로 근사하자는 것이 Variational Inference의 핵심적인 아이디어입니다. 이 때 물론 $$q(z)$$는 "tractable"한 분포로 설정해야 하겠죠. 예를 들면 Gaussian 분포처럼 말입니다. 이 분포를 앞서 구할 수 없었던 $$p(z\mid x)$$에 충분히 근사할 수 있다고 하면? 원하는 결과를 얻는 것이겠죠. 그럼 먼저 확률분포의 차이를 계산하기 위해 클백-라이블러 발산(Kullback-Leibler Divergence, KLD)을 활용합니다. KLD에 대해 보다 자세한 이론은 [순록킴님의 블로그 글](https://hyunw.kim/blog/2017/10/27/KL_divergence.html)을 참고하시기 바랍니다. 굉장히 쉽게 설명이 되어있습니다.<br><br>

KLD가 p(x)와 q(x)라는 분포의 차이를 구한다고 할 때 그 식은<br>
\begin{align}D_{KL}(p(x)\mid \mid q(x)) = \int{p(x)log\frac{p(x)}{q(x)}dx}\end{align}
가 됩니다. 이를 우리가 구하려는 값에 맞게 조금 변형하면<br>
\begin{align}D_{KL}(q(z\mid x)\mid \mid p(z\mid x)) = -\int{q(z\mid x)log\frac{p(z\mid x)}{q(z)}dz}\end{align}
가 될 수 있는데 이 때 $$p(z\mid x) = \frac{p(x\mid z)p(z)}{p(x)}=\frac{p(x,z)}{p(x)}$$가 됨을 이용하면,

$$\begin{align}D_{KL}(q(z\mid x)\mid \mid p(z\mid x))
&=-\int{q(z\mid x)log\frac{p(x,z)}{q(z\mid x)p(x)}dz}\\
&=-\int{q(z\mid x)[log\frac{p(x,z)}{q(z\mid x)}dz-logp(x)]dz}\\
&=-\int{q(z\mid x)log\frac{p(x,z)}{q(z\mid x)}dz+\int{q(z\mid x)logp(x)dz}}\\
&=-\int{q(z\mid x)log\frac{p(x,z)}{q(z\mid x)}}dz+logp(x)\end{align}$$<br><br>
가 됩니다. 긴 수식이지만, 쭉 보시면 이해하기는 어렵지 않습니다. 이제 굉장히 크리티컬한 수식이 도출될 수 있습니다.<br>
\begin{align}logp(x) = D_{KL}(q(z\mid x)\mid \mid p(z\mid x)) + \int{q(z\mid x)log\frac{p(x,z)}{q(z\mid x)}}dz\end{align}
우리는 애초에 $$q(z)$$라는 분포를 통해 구하기 어려운 $$p(z\mid x)$$에 근사하려 했습니다만, 결과론적으로 $$\log{p(x)}$$ 값을 추정할 수 있는 수식을 만들어 냈습니다. 또 한 가지 짚을 점은, $$\log{p(x)}$$는 $$q$$가 어떤 분포이든지 상관 없이 주어진 데이터 $$x$$와 어떤 분포 $$p(.)$$에 대해 constant로 취급될 수 있습니다. 우변의 좌항을 $$D$$, 우항을 $$L$$이라고 하면 우리의 원래 목적인 "$$D$$를 최소화" = "$$L$$의 최대화"가 됩니다.<br><br>
이 때 Jensen's inequality에 따라 $$D$$는 0보다 크거나 같은 값이 되므로, 결국 $$logp(x) \geq L$$이 됩니다. 그런 이유로 이 $$L = \int{q(z\mid x)\log\frac{p(x,z)}{q(z)}}$$을 (Variational) Lower Bound라고 부르고, 혹은 ELBO(Evidence Lower BOund)라고도 부릅니다. 중요해 보이는 $$L = \int{q(z\mid x)log\frac{p(x,z)}{q(z\mid x)}}$$를 조금 더 살펴보도록 하겠습니다.<br>

$$\begin{align}L = \int{q(z\mid x)log\frac{p(x,z)}{q(z\mid x)}}dz
&= \int{q(z\mid x)log\frac{p(x\mid z)p(z)}{q(z\mid x)}}dz\\
&= \int{q(z\mid x)logp(x\mid z)}dz + \int{q(z\mid x)log\frac{p(z)}{q(z\mid x)}}dz\\
&= E_{q(z\mid x)}logp(x\mid z) - D_{KL}(q(z\mid x)\mid \mid p(z))\end{align}$$<br>

마지막 등식의 첫번째 항은 Reconstruction likelihood를 의미하고, 이에 음수를 취한 $$-E_{q(z\mid x)}logp(x\mid z)$$는 원본 데이터 $$x$$와 복원된 데이터 $$x'$$ 간 $$cross-entropy$$가 됩니다. 딥러닝의 관점에서 위 값은 결국 학습을 통해 최소화 해야 하는 손실함수가 됩니다.<br><br>
두번째 항은 우리가 가정한 분포 $$q$$가 얼마나 prior인 $$p$$와 유사하게 학습되었는지를 의미합니다. VAE는 $$z$$의 prior, 즉 $$p(z)$$를 zero-mean Gaussian 분포($$N(0, 1)$$)를 따른다고 가정합니다. 즉 우리는 두개의 정규분포 $$q(z\mid x)$$와 $$p(z)$$ 간 KL-divergence를 계산하게 되는데, 이건 계산이 쉽다고 합니다(_그 이유는 논문의 appendix-B에 서술되어 있으나, 더 공부를 해봐야 할 것 같습니다_) 각 분포를 $$N(\mu, \sigma)$$, $$N(0, 1)$$로 가정한 뒤 KL-divergence를 계산하면 아래와 같은 식이 나옵니다.<br>

\begin{align}-\frac{1}{2}\Sigma(1 + \log(\sigma^{2})-\mu^{2}+\sigma^{2})\end{align}

다만 이 값은 또한 'Regularization Term'이라고 할 수 있는데, 만약 이 항이 없이 VAE를 학습한다면 어떻게 될까요? 아마 Autoencoder의 한계와 마찬가지로 단순히 reconstruction error를 줄이기 위해서만 학습된 모델이 될 겁니다. latent space의 특정 공간에만 값이 몰리는 등 input data를 'cheat'하여, 똑같이 복원(overfitted)하는 데에만 신경을 쓰게 되겠지요. VAE는 regularization term을 이용함으로써 샘플링되는 latent feature $$z$$가 충분히 Gaussian 분포를 따르도록, 그리고 그 안에서 stochastic하게 샘플링이 되도록 한다고 보시면 될 것 같습니다.<br><br>
코드 상에서 위 Loss를 설정하는 부분은 아래와 같습니다.
```python
def loss_function(x_org, x_recons, mu, logvar):
    '''
    VAE 모델의 목적함수(손실함수)
     (1) Recontruction Error: 원본 데이터 x_org와 복원된 데이터 x_recons 간 차이(정보손실) 측정
     (2) KLD Error: p(z\mid x)에 근사하려는 q(z\mid x)가 얼마나 잘 근사되는지 측정
                    이 때 가정하는 p(z\mid x)가 N(0, 1)이므로, q(z\mid x)가 N(0, 1)이 될 때 최소화 된다.
    '''
    REC_loss = F.binary_cross_entropy(x_recons, x_org, size_average = False)
    KLD_loss = -0.5 * torch.sum(1 + logvar - mu.pow(2) - torch.exp(logvar))
    tot_loss = (REC_loss + KLD_loss)
    return tot_loss
```
추가적으로 [Ratsgo님의 블로그](https://ratsgo.github.io/generative%20model/2018/01/27/VAE/)에 위 목적함수에 대해 굉장히 직관적으로 표현한 도식이 있으니, 한번 참고하시면 좋을 듯 합니다.<br>

![img](/assets/assets_post/2020-10-21-vae/vae_loss_function.png)<br><br>

## **Reparametrization Trick**<br>
지금까지 Variational Inference가 어떤 개념인지를 알아보았습니다. 결국 우리는 하나의 손실함수를 얻게 되었고, 복원이 잘되고 동시에 $$q$$가 $$p$$에 근접한 분포를 그린다면 우리가 원하는 결과를 얻을 수 있겠다는 느낌이 옵니다. VAE 구조를 다시 한 번 보겠습니다.<br><br>
![image](/assets/assets_post/2020-10-21-vae/vae_architecture.png)<br>
notation이 조금 다르지만, 어쨌든 우리는 $$q(z\mid x)$$라는 _Gaussian Encoder_ 를 학습하여 데이터 $$x$$로부터 바로 $$z$$를 만드는 것이 아니라 학습된 $$z$$의 분포(Gaussian)가 갖는 파라미터 $$(\mu, \sigma^{2})$$를 이용해 $$z$$를 샘플링하고자 합니다. 다만 한가지 문제가 생깁니다. 위 모델을 Gradient Descent 방식으로 학습시키려고 보니 $$z$$가 deterministic이 아닌 stochastic한 과정을 거쳐 생성된 "샘플된 값"이기 때문에 우리가 gradients를 backpropate 하기 어렵다는 데 있습니다. 이를 해결하기 위해 Reparametrization trick이라는, 참 기가 막힌 아이디어가 등장합니다.<br><br>
Reparametrization trick을 직관적으로 잘 설명하는 그림이 바로 아래 그림입니다. (출처: [MIT 강의](https://youtu.be/rZufA635dq4?t=1400))<br>

![img](/assets/assets_post/2020-10-21-vae/reparam_layers.png)<br>
원래 $$z$$가 $$N(\mu, \sigma^{2})$$에서 샘플된 값이기 때문에 random node로서 파라미터 $$\phi$$에 대한 미분이 불가능했습니다만, $$z$$ 대신 $$\epsilon\sim N(0, 1)$$을 샘플링하고, $$z' = \mu + \sigma\epsilon$$으로 대체하면 파라미터 $$[\mu, \sigma]$$에 대해 미분이 가능해집니다. 도식을 보시면 아시겠지만, $$z'$$의 샘플링이 이제는 고정된 $$\mu, \sigma$$를 이용하기 때문입니다. 참고로 $$\epsilon$$값은 충분히 작은 값이기 때문에, 네트워크 결과물에 큰 변형을 주지는 않는다고 합니다.<br><br>
코드 상으로 reparametrization을 구현하는 부분은 아래와 같습니다. torch.randn을 이용해 간단하게 $$\epsilon$$을 뽑아서 $$z$$를 만듭니다.
```python
def sample_q(self, mu, logvar):
   '''
   `앞선 neural network 구조에서 생성된 mu, logvar을 이용해 정규분포를 만들고, 이 정규분포로부터 z를 샘플링 (encode)
   `이 때 reparametrization trick을 위해 N(0, 1)로부터 epsilon을 샘플링
   '''
   std = torch.exp(logvar * 0.5)
   eps = torch.randn_like(std) # sampling epsilon
   Z = mu + std * eps
   return Z
```
<br>

## **학습** <br>
위 모든 내용을 사용해서 autoencoder와 같은 구조의 네트워크를 쌓게 됩니다. 코드로 구현된 학습 모델은 아래와 같습니다.<br>

```python
# Modeling Variational Autoencoder
class VAE(nn.Module):
    '''
    `VAE 모델
    (1) 데이터를 input으로 받고
    (2) encoder를 통해 latent variable로 변환 (샘플)
    (3) latent variable을 다시 decoder로 동일한 크기의 x' 생성
    
    `hyperparameter
    인코더와 디코더가 hidden <--> latent의 구조일 때
    각 layer의 크기 (hidden: 400, latent: 20)
    '''
    def __init__(self, input_dim = 784, hidden_dim = 400, latent_dim = 20):
        super(VAE, self).__init__()
        
        self.fc_encode = nn.Linear(input_dim, hidden_dim)
        self.fc_mean   = nn.Linear(hidden_dim, latent_dim)
        self.fc_logvar = nn.Linear(hidden_dim, latent_dim)
        
        self.fc_decoder1 = nn.Linear(latent_dim, hidden_dim)
        self.fc_decoder2 = nn.Linear(hidden_dim, input_dim)
    
    def encoder(self, to_encode):
        e1   = F.relu(self.fc_encode(to_encode.view(-1, 784)))
        mu   = self.fc_mean(e1)
        logvar  = self.fc_logvar(e1)
        return mu, logvar
    
    def decoder(self, to_decode):
        d1 = F.relu(self.fc_decoder1(to_decode))
        out = self.fc_decoder2(d1)
        out = F.sigmoid(out)
        return out


    def sample_q(self, mu, logvar):
        '''
        `앞선 neural network 구조에서 생성된 mu, logvar을 이용해 정규분포를 만들고,
        이 정규분포로부터 z를 샘플링 (encode)
        `이 때 reparametrization trick을 위해 N(0, 1)로부터 epsilon을 샘플링
        '''
        std = torch.exp(logvar * 0.5)
        eps = torch.randn_like(std) # sampling epsilon
        Z = mu + std * eps
        return Z
    
    def forward(self, x):
        z_mu, z_logvar = self.encoder(x)
        z = self.sample_q(z_mu, z_logvar)
        reconstructed = self.decoder(z)
        return reconstructed, z_mu, z_logvar
```
<br>

## **Result**
자, 그럼 돌아돌아 Variational Autoencoder가 어떤 모델인지 알아보았습니다. 설명만 보면 이게 어떤거구나 와닿지 않기 때문에, MNIST를 이용해 학습된 VAE를 여러 측면에서 살펴보도록 하겠습니다.<br><br>
**(1) 생성 결과물**<br>
우선 MNIST digits 데이터를 VAE에 학습시키되, latent dimension을 변경해서 학습을 진행하였습니다. 직관적으로 latent dimension이 크면 원본 이미지와 더 가깝게 "뚜렷한" 이미지를 생성할 것이라 추측할 수 있습니다. 한 번 볼까요?<br><br>
[원본 이미지]<br>
![image](/assets/assets_post/2020-10-21-vae/mnist_vis_org.png)<br><br>
[latent dimension = 20]<br>
![image](/assets/assets_post/2020-10-21-vae/mnist_vis_recon_20.png)<br><br>
[latent dimension = 5]<br>
![image](/assets/assets_post/2020-10-21-vae/mnist_vis_recon_5.png)<br>
latent dimension이 5인 경우 20인 경우보다 조금 흐릿한 형태로 복원된 것을 볼 수 있습니다. 특히 첫번째 줄의 7, 8번째 개체인 "3", "8"의 경우 latent dimension = 5는 이를 "8", "9"로 바꾸어 복원했네요. 다만 5개의 차원만을 사용했음에도 충분히 좋은 성능을 보이는 것을 볼 수 있습니다.<br><br>
**(2) Encoding 결과 시각화**<br>
2차원인 latent dimension를 이용해 VAE를 학습시키고, encoder가 뱉는 latent space를 시각화 한 결과는 아래와 같습니다.<br>
![image](/assets/assets_post/2020-10-21-vae/vis_mu_scatter.png)<br><br>
플롯의 x축과 y축은 차원을 생성하는 데 사용한 정규분포의 평균($$\mu_{i}$$)가 되겠습니다. 우선 공간 상에 숫자들이 어느 정도 군집을 생성하며 잘 분포되어 있는 것을 볼 수 있습니다. 이를 가지고 "$$28*28$$인 MNIST 데이터의 manifold를 잘 학습했다"라고도 해석할 수 있겠습니다. 또한 $$\mu$$값이 굉장히 넓게 퍼져있는 것을 알 수 있는데, 이를 통해 샘플링의 이용하는 VAE의 강점을 알 수 있습니다. 어떤 임의의 값을 집어넣어도, 이후의 decoder가 충분히 좋은 결과물을 생성할 수 있겠다는 점입니다. 이는 바로 아래 결과로 확인할 수 있습니다.<br><br>
**(3) Decoding 결과 시각화**<br>
latent feature를 decoder로 복원한 결과물을 한 번 살펴보겠습니다. (2)와 마찬가지로 2차원의 $$z$$를 학습시키고, 제가 맘대로 설정한 값을 넣어 decoding을 진행해 본 결과는 아래와 같습니다.<br>
(_1번: $$[0.5, 0]$$, 2번: $$[2, 0]$$_)<br>
![image](/assets/assets_post/2020-10-21-vae/mnist_8.png)
![image](/assets/assets_post/2020-10-21-vae/mnist_3.png)<br>
참 신기하게도 아무 값이나 넣었을 때 충분히 "숫자 같은" 이미지를 생성해 내는 것을 볼 수 있습니다. 이는 물론 VAE가 encoding과 decoding에 확률분포를 사용하기 때문이겠죠. 위 결과를 좀더 확장해서, 엄청 많은 임의의 값에 대한 복원 결과물을 살펴볼까요?<br><br>
![image](/assets/assets_post/2020-10-21-vae/mnist_vis_decoded.png)<br>
우리가 충분히 인지할 수 있는 숫자들을 잘 생성합니다. 특히 latent space에 유사한 숫자들끼리 모여있는 형상을 확인할 수 있습니다.<br><br> 

## **Summary**
긴 글이지만 VAE의 프로세스를 간략하게 정리를 해보면 ,<br>
1. Autoencoder와 유사한 구조의 [_data-encoder-decoder-data'_] 모델을 구축합니다.
2. 이 때 모델을 학습하기 위한 Loss는 1) Reconstruction Error + 2) Regularization입니다.
3. 데이터 $$x$$에 대해 사후확률 $$p(z\mid x)$$를 Encoder로 사용해 데이터 $$x$$의 manifold를 학습하여 latent variable인 $$z$$를 생성(샘플링)합니다. 이 때 대부분의 경우 $$p(z\mid x)$$를 알 수 없으므로, Variational Inference를 사용해 $$q(z\mid x)\sim p(z\mid x)$$인 $$q(z\mid x)$$를 추정합니다. (Regularization)
4. Latent feature $$z$$로부터 $$x'$$를 샘플링합니다. 이는 $$p(x\mid z)$$라는 확률분포를 이용하게 되고, 우리는 $$x$$와 $$x'$$가 서로 충분히 유사하도록 $$E_{q(z\mid x)}logp(x\mid z)$$를 최대화합니다. (Reconstruction)
5. 이 때 backpropagation을 이용해 end-to-end로 학습시키기 위해 $$z$$를 $$q(z\mid x)$$에서 직접 샘플링하는 것이 아니라, $$\epsilon$$을 샘플링하고 $$z = \mu + \sigma \epsilon$$를 샘플링합니다.
6. Regularization + Reconstruction를 목적함수로 사용하여, 우리가 잘 아는 neural network 관점에서 VAE를 학습합니다.<br>

앞서도 말씀드렸듯이 VAE는 단순히 $$x$$를 $$z$$로, $$z$$를 $$x$$로 복원하는 기존 autoencoder의 특성에 stochastic한 sampling 구조를 추가하여 latent variable의 변형에도 어느 정도 robust한, 그리고 무언가 '다른' 데이터 $$x'$$를 만들어내는 관점에서 강점이 있습니다.<br><br>
반면 주요 단점으로 출력이 선명하지 않고 blurry 하다는 점이 꼽힙니다. 이는 아무래도 VAE가 stochastic하고, 샘플링 자체가 $$\epsilon$$라는 랜덤 노이즈로부터 이루어지기 때문이 아닐까 싶습니다. 2017년의 한 [논문](https://arxiv.org/pdf/1702.08658.pdf)은 이러한 점을 해결하기 위해 Variational Bayes를 사용하지 않는 방안을 제시했다고 하는데, 시간이 되면 한 번 읽어봐야 하겠습니다.<br><br>

긴 포스팅 읽어주셔서 너무나 감사합니다. 배울 것이 많은 학생이라 제가 해석한 내용 중 틀린 부분이나 보완이 필요한 부분이 있을 수 있습니다. 가르침 주시기 위한 피드백은 언제나 환영입니다.<br><br><br>

### **Reference**<br>

[유재준님의 블로그 VAE 설명글] http://jaejunyoo.blogspot.com/2017/04/auto-encoding-variational-bayes-vae-1.html<br>
[Ratsgo님의 블로그 VAE 설명글] https://ratsgo.github.io/generative%20model/2018/01/27/VAE/<br>
[차준범님의 VAE 설명 영상] https://youtu.be/KYA-GEhObIs<br>
[Variational autoencoders.] https://www.jeremyjordan.me/variational-autoencoders/<br>
[Lecture of Ali Ghodsi] https://youtu.be/uaaqyVS9-rM<br>
[Lecture from MIT, Deep Generative Modeling] https://youtu.be/rZufA635dq4<br>
