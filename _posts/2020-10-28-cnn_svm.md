---
layout: post
title:  "An Architecture Combining Convolutional Neural Network and Support Vector Machine for Image Classification"
date:   2020-10-28 14:25:52
author: mons2us
categories: Paper-Reproduction Deeplearning
tags: deeplearning svm
---
> 이 글은 논문 \<An Architecture Combining Convolutional Neural Network and Support Vector Machine for Image Classification\>를 이해하고자 다양한 소스로부터 학습한 내용을 정리한 글입니다. 혹시 내용 중 틀린 부분이 있거나 궁금한 사항이 있으신 경우 댓글을 남겨주시면 최선을 다해 소통하겠습니다.<br>
위 논문을 재현(reproduce)한 결과물은 제 [깃허브](https://github.com/mons2us/paper_reproduction/tree/master/Autoencoding-Variational-Bayes)를 참고하시기 바랍니다.

## Overview
우리는 많은 경우 classification을 위한 딥러닝 모델에서 마지막 layer(많은 경우 fully-connected layer)를 가지고 softmax cross entropy를 계산합니다. Softmax cross entropy는 직관적이고 효과적이며, 무엇보다 연산이 빠릅니다.
하지만 "softmax가 가장 낫다"라는 논리에 반박하기 위한 연구도 있습니다. 대표적으로 \<Deep Learning using Linear Support Vector Machines, Yichuan Tang, 2013\>의 저자는 위 softmax 대신에 linear support vector machine(L2-SVM)을 집어넣어 MNIST, CIFAR-10 등의 데이터셋 분류 문제에서 더 나은 성능을 얻은 바 있습니다. "분류 모델에는 (거의) 무조건 softmax"라고 생각했던지라 적잖이 놀라운 결과였습니다. 무엇보다 loss를 학습하기 위한 최적의 목적함수가 데이터에 따라, 상황에 따라 다를 수 있다는 것을 다시 한 번 깨닫게 되었죠.
<br><br>
본 포스트에서 소개할 논문 \<An Architecture Combining Convolutional Neural Network and Support Vector Machine for Image Classification\>는 위 \<Deep Learning using Linear Support Vector Machines\>의 후속 논문입니다.
앞선 논문을 CNN 모델의 관점에서 좀 더 자세하게 분석한 결과라고 할 수 있는데 문제는... \<Deep Learning using Linear Support Vector Machines\>는 "CNN+SVM이 SNN+Softmax보다 낫다"는 실험 결과를 보여준 반면 \<An Architecture Combining Convolutional Neural Network and Support Vector Machine for Image Classification\>의 실험 결과는 그와 반대입니다. 약간 웃픈 상황이기는 하나 어쨌든 후자 논문은 conclusion에서 아래와 같이 말합니다.<br>
Despite its contradiction to the findings in [11], quantitatively speaking, the test accuracies of CNN-Softmax and CNN-SVM are almost the same with the related study.
It is hypothesized that with data preprocessing and a relatively more sophisticated base CNN model, the results in [11] shall be reproduced.
<br>
즉 소개드리는 논문에서의 CNN은 굉장히 심플한 2-layer Conv Layer을 이용하고 있고, \<Deep Learning using Linear Support Vector Machines\>에서는 좀 더 정교한 모델을 썼기에 위와 같은 반대되는 결과가 발생했다고 하는 듯 합니다. 근데 CNN모델을 더 정교화하면 softmax를 이용한 결과도 상향되지 않을까요? ㅎㅎ 어쨌든 본 논문은 내용을 순차적으로 다룬 후에 실험에 사용한 코드 구현을 수행하도록 하겠습니다.
<br><br>

## 왜 SVM인가?
논문 \<An Architecture Combining Convolutional Neural Network and Support Vector Machine for Image Classification\>의 저자는 Introduction 부분에서 아래와 같이 말합니다.<br>
> However, there have been studies conducted that takes a look at an alternative to softmax function for classification – the support vector machine (SVM). The aforementioned studies have claimed that the use of SVM in an artificial neural network (ANN) architecture produces a relatively better results than the use of the conventional softmax function. Of course, there is a drawback to this approach, and that is the restriction to binary classification.<br>

기본적으로 이진 분류 모델인 SVM이기 때문에, binary classfication 문제에 한해서 softmax보다 svm이 나은 결과를 가져올 수 있다는 내용입니다. 실제로 SVM은 굉장히 간단하지만 꽤나 높은 차원에서도 효과적인 방법론이기 때문에 머신러닝에서 큰 입지를 차지하고 있기도 합니다. 다만 클래스가 여러 개인 경우는 one-versus-rest 문제가 되어 target class가 1, 나머지는 모두 -1이 됩니다. 아무래도 이러한 multi-class 문제에서는 softmax가 성능이 좋을 것이라 생각됩니다만, 어쨌든 논문을 계속 살펴보겠습니다.
<br><br>

## SVM이 무엇인가?
그렇다면 Support Vector Machine(SVM)이 실제로 어떤 방법론인지 이론적인 부분을 살펴보도록 하겠습니다. SVM은 Vapnik에 의하여 binary classification을 위해 고안된 방법론으로서, 그 목적은 hyper parameter인 $$f(w, x)$$를 찾아 데이터셋의 두 클래스를 분류하는 데에 있습니다.<br><br>
![img](/assets/assets_post/2020-10-28-cnn_svm/svm_margin.png)<br>
위 그림은 [위키피디아](https://en.wikipedia.org/wiki/Support_vector_machine)에서 support vector machine을 설명하는 도식입니다. 파랑색과 초록색의 데이터가 공간 상에 분포하고 있다고 할 때, 그 둘을 (최대한 잘) 갈라놓는 $$w^{T}x-b=0$$을 찾습니다. 그러한 직선(혹은 평면)이 위아래를 갖고 있다고 할 때(+, -) 위에 있는 데이터는 +그룹, 아래에 있는 데이터는 -그룹으로 분류하는 게 linear SVM의 목적입니다. 또한 여기서 마진(margin)의 개념이 중요한데, 마진이란 각 그룹의 데이터로부터 분류 경계면까지의 거리 중 가장 짧은 것을 의미합니다. 직관적으로 이 마진이 클수록 깔끔하게 잘 분류하고 있다고 판단할 수 있습니다. <br><br>
조금 더 수식적으로 접근하면, 2차원 공간 상에서 직선 $$w^{T}x-b$$이 $$w_{1}x_{11}+w_{2}x_{12}-b$$로 정의된다고 해봅시다. 어떠한 데이터 $$X_1(x_{11}, x_{12})$$와 해당 직선 사이의 최단 거리는 $$\frac{|w_{1}x_{11}+w_{2}x_{12}-b|}{\sqrt{w^{2}_{1}+w^{2}_{2}}}$$가 됩니다.<br>

위 그림에서 파란색 데이터에 걸쳐져 있는 $$w^{T}x-b=1$$를 마진이라고 하고 이 마진에 $$X_{1}$$이 놓여져 있었다고 하면,
$$\frac{|w_{1}x_{11}+w_{2}x_{12}-b|}{\sqrt{w^{2}_{1}+w^{2}_{2}}}=\frac{1}{\sqrt{w^{2}_{1}+w^{2}_{2}}}=\frac{1}{\|w\|}$$가 됩니다. 두 마진 (+, -)를 모두 고려한다고 하면 위 도식의 $$\frac{2}{\|w\|}$$를 얻을 수 있습니다. 이 마진을 최대화하는 $$max\frac{2}{\|w\|}$$는 라그랑지안 승수법(Lagrange multiplier method)를 통해 구할 수 있습니다만, 본 포스트에서는 여기까지 다루지는 않습니다. SVM에 관한 좀 더 자세한 포스트에서 위 내용을 제대로 다루도록 하겠습니다.<br><br>
결과적으로 linear SVM은 직선(평면)을 이용해 마진을 최대화하는 분류 경계면을 만들게 되고, 만일 클래스가 두 개가 아닌 여러 개(multi-class)인 문제라면 one-versus-rest 문제로 풀게 됩니다. 즉 타겟 클래스가 $$y_{1}$$이면, 나머지 $$y_{i}, i \not ={1}$$를 모두 -1로 두고 분류를 수행합니다.

## Hinge Loss
논문에 따르면 위에 설명드린 수식 $$f(w, x)={w}\cdot{x}+b$$에서 최적의 $$w$$를 찾기 위해 아래와 같은 optimization 문제를 풀게 됩니다.<br>
$$\begin{align}min\frac{1}{p}\|w\|^{2}_{2}+C \sum_{i=1}^{p} {max(0, 1-y_{i}(w^{T}x_{i}+b))^{2}}\end{align}$$
<br>
좌항은 L2 norm을 이용한 normalization 항이고, 우측은 squared hinge loss가 됩니다. SVM을 최적화 하는 데에 hinge loss가 중요하기 때문에 개념을 한 번 살펴보도록 하겠습니다.<br><br>
hinge loss는 SVM 모형을 학습하기 위해 사용하는 목적함수로서 클래스들 간의 거리를 최대화하는 방향으로 분류를 수행하게 됩니다. 수식에 포함되는 1은 safety margin으로 설정되는 값입니다. 만약 분류 모형에서 실제 클래스가 $$i$$라고 할 때, $$i$$로 예측하는 스코어 $$p_{i}$$와 $$j$$로 예측하는 스코어 $$p_{j}$$ 간 차이인 $$p_{i}-p_{j}$$가 1보다 크지 않다면 $$max(0, 1-p_{i}-p_{j})$$에 의해 loss가 존재하게 됩니다. 반면 차이가 1보다 크다면 모델이 클래스 $$i$$로 잘 예측했다는 뜻이 되고, 동시에 loss가 0이 됩니다. hinge loss는 SVM 뿐만 아니라 GAN이나 이미지 분류에 있어서도 많이 쓰이는 loss입니다. 보다 자세한 내용은 [cs231n 강의노트](https://cs231n.github.io/linear-classify/)를 참고하시면 좋을 것 같습니다.

## CNN
CNN은 워낙 유명한 네트워크다보니 기본적인 것들만 짚고 넘어가도록 하겠습니다. Convoltional Neural Network(CNN)는 보통의 뉴럴 네트워크와 기본적으로 동일합니다. 학습이 가능한 뉴런들을 가지고 인풋(이미지)을 받은 후에, 내적을 하고 다양한 비선형성(ReLU, Sigmoid 등)을 추가합니다.
(_참고로 Convolution이라는 용어는 kernel(filter)들이 matrix 형태로써 이미지의 각 픽셀값을 합성곱(Convolution)한 결과를 이용하기 때문에 지어진 이름입니다_)
CNN을 보통 이미지 연산에 사용하기는 하지만, 이미지 분야 외에도 음성, 문자, 시계열 등 다양한 데이터에 사용하는 딥러닝 기법입니다.<br>
![img](/assets/assets_post/2020-10-28-cnn_svm/cnn_archi.jpg)<br>
위 그림이 이미지 분류에 있어서 CNN을 설명하는 대표적인 도식입니다. 본 논문이 classification 문제이기 때문에 해당 관점으로 설명을 해보면, 보편적인  Image Classification 문제에서 우리는<br>
> 1) 어떤 이미지를 받고<br>
2) CNN에 태운 후에<br>
3) Fully connected layer(Dense layer)에 태워 결과물을 벡터 형태로 만듭니다.
<br>

이 때 class 개수가 80이라면 최종적인 벡터의 개수도 80이 될 것이고, 해당 벡터를 다시 softmax 함수에 태워서 cross entropy를 계산하는 것이 일반적인 softmax-cross entropy입니다. 소개해드리는 논문은 최종 벡터를 가지고 softmax 함수가 아니라 위에 소개한 hinge loss 함수를 계산합니다. one-versus-rest의 문제로 만약 input 이미지가 2라면, 2로 예측한 score를 $$i$$, 나머지 score를 $$j_{k}$$로 하여 각각의 hinge loss를 구하고 평균을 계산하게 됩니다.<br><br>

# Experiment (Reproduction)
> 본 논문의 실험 결과를 재현하기 위해 pytorch를 사용하였습니다.<br>

## 방법론
우선 논문의 실험을 위해 가장 대표적 분류 데이터인 MNIST digit dataset을 사용합니다. 하지만 이는 너무 쉬운 데이터셋으로서 현대의 Computer Vision을 대표하기엔 어려운 데이터입니다. 매우 얕은 구조로도 99% 정확도는 쉽게 달성하기 때문입니다.
그래서 논문의 저자는 Fashion-MNIST를 추가적으로 실험에 이용하였습니다. 데이터 분포는 아래와 같습니다.<br><br>
![img](/assets/assets_post/2020-10-28-cnn_svm/dataset_dist.png)<br>
개인적으로는 Fashion-MNIST도 충분히 쉬운 데이터셋이라 CIFAR-100, 하다못해 CIFAR-10 정도의 데이터셋으로 실험을 해보면 어떨까 하는 궁금증이 있습니다. 이 부분에 대해서는 논문 리뷰 후에 추가적으로 진행을 해볼 계획입니다.<br><br>

## 구조
우선 실험에 사용되는 CNN 구조는 아래와 같습니다.
<br>
> (1) INPUT: 32 × 32 × 1<br>
(2) CONV5: 5 × 5 size, 32 filters, 1 stride<br>
(3) ReLU: max(0,hθ(x))<br>
(4) POOL: 2 × 2 size, 1 stride<br>
(5) CONV5: 5 × 5 size, 64 filters, 1 stride<br>
(6) ReLU: max(0,hθ(x))<br>
(7) POOL: 2 × 2 size, 1 stride<br>
(8) FC: 1024 Hidden Neurons<br>
(9) DROPOUT: p = 0.5<br>
(10) FC: 10 Output Classes<br>

보시듯이 굉장히 간단한 2-layer CNN을 이용하였고, 과적합을 방지하기 위해 첫번째 fully connected layer 뒤에 dropout(p=0.5)를 추가했음을 확인할 수 있습니다. 모델을 구축한 코드는 아래와 같습니다.<br>

```python
class CNN(nn.Module):
    
    def __init__(self, in_channels = 1, class_num = 10):
        super(CNN, self).__init__()

        self.class_num = class_num
    
        # Common Function
        self.maxpool_22 = nn.MaxPool2d(kernel_size=(2, 2))
        self.relu = nn.ReLU()
        
        # Block 1
        b1_conv1 = nn.Conv2d(in_channels = in_channels, out_channels = 32, kernel_size = 5, stride = 1, padding = 2)
        
        # Res: (28-5+4)/1 + 1 = 32 -> 14
        self.b1_block = nn.Sequential(b1_conv1,
                                      nn.ReLU(),
                                      self.maxpool_22)
        
        # Block 2
        b2_conv1 = nn.Conv2d(in_channels = 32, out_channels = 64, kernel_size = 5, stride = 1, padding = 2)
        
        # res: (14-5+4)/1 + 1 = 16 -> 7
        self.b2_block = nn.Sequential(b2_conv1,
                                      nn.ReLU(),
                                      self.maxpool_22)
        
        # FC layer
        fc_1 = nn.Linear(7 * 7 * 64, 1024)
        self.fc_block = nn.Sequential(fc_1,
                                     nn.Dropout(p = 0.5))

        self.fc_last = nn.Linear(1024, self.class_num)
        
    def forward(self, x):
        # Conv Layers
        out = self.b1_block(x)
        out = self.b2_block(out)

        # Calculate tensor size after all conv layers
        dim = torch.prod(torch.tensor(out.size()[1:]), 0)
        out = out.view(-1, dim)

        # Fully connected layers
        out = self.fc_block(out)
        out = self.fc_last(out)
        
        return out, self.fc_last.weight
    
cnn_model = CNN()
```
<br>
loss를 계산하기 위한 함수는 loss_softmax()와 loss_svm() 두 가지입니다. torch는 multi-class hinge loss를 MultiMarginLoss()라는 함수로 구현하고 있습니다. softmax는 당연히 nn.CrossEntropyLoss()를 사용하며, 각 loss를 코드로 구현한 결과는 아래와 같습니다.
```python
class loss_softmax(nn.Module):
    '''
    Softmax를 사용할 경우의 loss함수
    '''
    def __init__(self):
        super(loss_softmax, self).__init__()
    
    def forward(self, pred, gt, reg_term, device, current_batch_size):
        softmax_loss = nn.CrossEntropyLoss()
        loss = softmax_loss(pred, gt)
        
        return loss
```
```python
class loss_svm(nn.Module):
    '''
    SVM을 사용할 경우의 loss함수
    '''
    def __init__(self, penalty_param = 1):
        super(loss_svm, self).__init__()
        self.penalty_param = penalty_param
    
    def forward(self, pred, gt, reg_term, device, current_batch_size):
        hinge_loss = nn.MultiMarginLoss(p = 2, margin = 1)

        # L2 Regularization term
        reg_loss = torch.mean(torch.square(reg_term))

        loss = self.penalty_param * reg_loss + hinge_loss(pred, gt)
        
        return loss
```
<br>

## 결과
논문의 test 결과와 재현한 코드로 수행한 test 결과는 아래와 같습니다.<br>

[논문]
![img](/assets/assets_post/2020-10-28-cnn_svm/paper_test_result.png)<br>

[재현]
```bash
# bash
# MNIST
python main.py --mode=test --loss_type=softmax --data_type=MNIST      
Test Accuracy: 0.9921
python main.py --mode=test --loss_type=svm --data_type=MNIST    
Test Accuracy: 0.9881

# Fashion-MNIST
python main.py --mode=test --loss_type=softmax --data_type=Fashion_MNIST             
Test Accuracy: 0.9125
python main.py --mode=test --loss_type=svm --data_type=Fashion_MNIST  
Test Accuracy: 0.9083
```
Fashion MNIST에 대한 test accuracy가 약간 낮게 재현되었지만, 논문의 주장과 마찬가지로 SVM을 이용한 모형이 Softmax에 비해 조금 부족한 정확도를 보임을 알 수 있습니다.<br>

## Conclusions
오늘 소개드린 논문 \<An Architecture Combining Convolutional Neural Network and Support Vector Machine for Image Classification\>는 통상적인 접근과 달리 MNIST, Fashion MNIST 데이터셋 분류 모델에 SVM을 이용하는 방법론을 보여줍니다. 함수의 특성 상 위와 같은 multi-class 문제에 있어서는 one-versus-rest 접근 방법을 택하는 SVM 보다는, 다수 클래스가 jointly 고려되는 softmax가 더 나은 결과를 가져오는 듯 싶습니다.<br><br>
하지만 SVM을 통한 분류 또한 충분히 softmax에 비견할 만한 성능을 보여줬기 때문에 딥러닝 모델에서 상황에 맞는 다양한 loss를 고려해야 한다는 점에서는 의미를 주고 있다고 생각합니다. 다만 앞서 서술했듯 MNIST 데이터가 아닌 좀 더 어려운 task의 dataset에 위 논문 접근법을 적용해보면 어떨까 하는 궁금증이 남습니다.<br><br>

긴 포스팅 읽어주셔서 너무나 감사합니다. 배울 것이 많은 학생이라 제가 해석한 내용 중 틀린 부분이나 보완이 필요한 부분이 있을 수 있습니다. 가르침 주시기 위한 피드백은 언제나 환영입니다.<br><br><br>